{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GckBEAe6p2SF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5121e056-6d85-42e8-e095-48c1dc935e14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rtdl\n",
            "  Downloading rtdl-0.0.13-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: torch<2,>=1.7 in /usr/local/lib/python3.7/dist-packages (from rtdl) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy<2,>=1.18 in /usr/local/lib/python3.7/dist-packages (from rtdl) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.7->rtdl) (4.2.0)\n",
            "Installing collected packages: rtdl\n",
            "Successfully installed rtdl-0.0.13\n",
            "Collecting libzero==0.0.4\n",
            "  Downloading libzero-0.0.4-py3-none-any.whl (26 kB)\n",
            "Collecting pynvml<9,>=8.0\n",
            "  Downloading pynvml-8.0.4-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: tqdm<5,>=4.0 in /usr/local/lib/python3.7/dist-packages (from libzero==0.0.4) (4.64.0)\n",
            "Requirement already satisfied: numpy<2,>=1.17 in /usr/local/lib/python3.7/dist-packages (from libzero==0.0.4) (1.21.6)\n",
            "Requirement already satisfied: torch<2,>=1.6 in /usr/local/lib/python3.7/dist-packages (from libzero==0.0.4) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.6->libzero==0.0.4) (4.2.0)\n",
            "Installing collected packages: pynvml, libzero\n",
            "Successfully installed libzero-0.0.4 pynvml-8.0.4\n"
          ]
        }
      ],
      "source": [
        "# Requirements:\n",
        "!pip install rtdl\n",
        "!pip install libzero==0.0.4"
      ],
      "id": "GckBEAe6p2SF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeRC47Alp2SI"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict\n",
        "\n",
        "import numpy as np\n",
        "import rtdl\n",
        "import scipy.special\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "import sklearn.model_selection\n",
        "import sklearn.preprocessing\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import zero\n",
        "from sklearn.metrics import f1_score"
      ],
      "id": "BeRC47Alp2SI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Hy1rMMKp2SJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04407278-675c-477b-d1e9-3e02dacbb1c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "123456"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "device = torch.device('cpu')\n",
        "# Docs: https://yura52.github.io/zero/0.0.4/reference/api/zero.improve_reproducibility.html\n",
        "zero.improve_reproducibility(seed=123456)"
      ],
      "id": "5Hy1rMMKp2SJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJv0e43qp2SK"
      },
      "source": [
        "### Data"
      ],
      "id": "pJv0e43qp2SK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEeOJSKmp2SL"
      },
      "outputs": [],
      "source": [
        "# !!! NOTE !!! The dataset splits, preprocessing and other details are\n",
        "# significantly different from those used in the\n",
        "# paper \"Revisiting Deep Learning Models for Tabular Data\",\n",
        "# so the results will be different from the reported in the paper.\n",
        "\n",
        "dataset = sklearn.datasets.fetch_california_housing()\n",
        "task_type = 'regression'\n",
        "\n",
        "# dataset = sklearn.datasets.fetch_covtype()\n",
        "# task_type = 'multiclass'\n",
        "\n",
        "assert task_type in ['binclass', 'multiclass', 'regression']\n",
        "\n",
        "X_all = dataset['data'].astype('float32')\n",
        "y_all = dataset['target'].astype('float32' if task_type == 'regression' else 'int64')\n",
        "if task_type != 'regression':\n",
        "    y_all = sklearn.preprocessing.LabelEncoder().fit_transform(y_all).astype('int64')\n",
        "n_classes = int(max(y_all)) + 1 if task_type == 'multiclass' else None\n",
        "\n",
        "X = {}\n",
        "y = {}\n",
        "X['train'], X['test'], y['train'], y['test'] = sklearn.model_selection.train_test_split(\n",
        "    X_all, y_all, train_size=0.8\n",
        ")\n",
        "X['train'], X['val'], y['train'], y['val'] = sklearn.model_selection.train_test_split(\n",
        "    X['train'], y['train'], train_size=0.8\n",
        ")\n",
        "\n",
        "# not the best way to preprocess features, but enough for the demonstration\n",
        "preprocess = sklearn.preprocessing.StandardScaler().fit(X['train'])\n",
        "X = {\n",
        "    k: torch.tensor(preprocess.fit_transform(v), device=device)\n",
        "    for k, v in X.items()\n",
        "}\n",
        "y = {k: torch.tensor(v, device=device) for k, v in y.items()}\n",
        "\n",
        "# !!! CRUCIAL for neural networks when solving regression problems !!!\n",
        "if task_type == 'regression':\n",
        "    y_mean = y['train'].mean().item()\n",
        "    y_std = y['train'].std().item()\n",
        "    y = {k: (v - y_mean) / y_std for k, v in y.items()}\n",
        "else:\n",
        "    y_std = y_mean = None\n",
        "\n",
        "if task_type != 'multiclass':\n",
        "    y = {k: v.float() for k, v in y.items()}"
      ],
      "id": "JEeOJSKmp2SL"
    },
    {
      "cell_type": "code",
      "source": [
        "task_type = 'binclass'"
      ],
      "metadata": {
        "id": "6ThomVE5sFJT"
      },
      "id": "6ThomVE5sFJT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_all = np.load('data/sf/x.npy')\n",
        "y_all = np.load('data/sf/y.npy')"
      ],
      "metadata": {
        "id": "cIEe72n6sB4A"
      },
      "id": "cIEe72n6sB4A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "def scale(X):\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X)\n",
        "    X = scaler.transform(X)\n",
        "    return X, scaler\n",
        "\n",
        "def data_balance(X, y):\n",
        "    sm = SMOTE()\n",
        "    X_res, y_res = sm.fit_resample(X, y)\n",
        "    return X_res, y_res\n",
        "\n",
        "\n",
        "X_train, scaler = scale(X_all)\n",
        "X_train, y_train = data_balance(X_all, y_all)\n",
        "\n",
        "X = {}\n",
        "y = {}\n",
        "X['train'], X['test'], y['train'], y['test'] = sklearn.model_selection.train_test_split(\n",
        "    X_all, y_all, train_size=0.8\n",
        ")\n",
        "X['train'], X['val'], y['train'], y['val'] = sklearn.model_selection.train_test_split(\n",
        "    X['train'], y['train'], train_size=0.8\n",
        ")\n",
        "\n",
        "X = {k: torch.tensor(v.astype('float32'), device=device) for k, v in X.items()}\n",
        "y = {k: torch.tensor(v.astype('int64'), device=device) for k, v in y.items()}\n",
        "\n",
        "if task_type == 'regression':\n",
        "    y_mean = y['train'].mean().item()\n",
        "    y_std = y['train'].std().item()\n",
        "    y = {k: (v - y_mean) / y_std for k, v in y.items()}\n",
        "else:\n",
        "    y_std = y_mean = None\n",
        "\n",
        "if task_type != 'multiclass':\n",
        "    y = {k: v.float() for k, v in y.items()}\n",
        "\n"
      ],
      "metadata": {
        "id": "VKKQEAcTscrK"
      },
      "id": "VKKQEAcTscrK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5JwxWR5p2SN"
      },
      "source": [
        "### Model\n",
        "Carefully read the comments and uncomment the code for the model you want to test."
      ],
      "id": "v5JwxWR5p2SN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AARlx-9Np2SN"
      },
      "outputs": [],
      "source": [
        "n_classes= 1\n",
        "d_out = n_classes or 1\n",
        "\n",
        "# model = rtdl.MLP.make_baseline(\n",
        "#     d_in=X_all.shape[1],\n",
        "#     d_layers=[128, 256, 128],\n",
        "#     dropout=0.1,\n",
        "#     d_out=d_out,\n",
        "# )\n",
        "# lr = 0.001\n",
        "# weight_decay = 0.0\n",
        "\n",
        "# model = rtdl.ResNet.make_baseline(\n",
        "#     d_in=X_all.shape[1],\n",
        "#     d_main=128,\n",
        "#     d_intermidiate=256,\n",
        "#     dropout_first=0.2,\n",
        "#     dropout_second=0.0,\n",
        "#     n_blocks=2,\n",
        "#     d_out=d_out,\n",
        "# )\n",
        "# lr = 0.001\n",
        "# weight_decay = 0.0\n",
        "\n",
        "model = rtdl.FTTransformer.make_default(\n",
        "    n_num_features=X_all.shape[1],\n",
        "    cat_cardinalities=None,\n",
        "    last_layer_query_idx=[-1],  # it makes the model faster and does NOT affect its output\n",
        "    d_out=d_out,\n",
        ")\n",
        "\n",
        "# === ABOUT CATEGORICAL FEATURES ===\n",
        "# IF you use MLP, ResNet or any other simple feed-forward model (NOT transformer-based model)\n",
        "# AND there are categorical features\n",
        "# THEN you have to implement a wrapper that handles categorical features.\n",
        "# The example below demonstrates how it can be achieved using rtdl.CategoricalFeatureTokenizer.\n",
        "# ==================================\n",
        "# 1. When you have both numerical and categorical features, you should prepare you data like this:\n",
        "#    (X_num<float32>, X_cat<int64>) instead of X<float32>\n",
        "#    Each column in X_cat should contain values within the range from 0 to <(the number of unique values in column) - 1>;\n",
        "#    use sklean.preprocessing.OrdinalEncoder to achieve this;\n",
        "# 2. Prepare a list of so called \"cardinalities\":\n",
        "#    cardinalities[i] = <the number of unique values of the i-th categorical feature>\n",
        "# 3. See the commented example below and adapt it for your needs.\n",
        "#\n",
        "# class Model(nn.Module):\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         n_num_features: int,\n",
        "#         cat_tokenizer: rtdl.CategoricalFeatureTokenizer,\n",
        "#         mlp_kwargs: Dict[str, Any],\n",
        "#     ):\n",
        "#         super().__init__()\n",
        "#         self.cat_tokenizer = cat_tokenizer\n",
        "#         self.model = rtdl.MLP.make_baseline(\n",
        "#             d_in=n_num_features + cat_tokenizer.n_tokens * cat_tokenizer.d_token,\n",
        "#             **mlp_kwargs,\n",
        "#         )\n",
        "#\n",
        "#     def forward(self, x_num, x_cat):\n",
        "#         return self.model(\n",
        "#             torch.cat([x_num, self.cat_tokenizer(x_cat).flatten(1, -1)], dim=1)\n",
        "#         )\n",
        "#\n",
        "# model = Model(\n",
        "#     # `None` means \"Do not transform numerical features\"\n",
        "#     # `d_token` is the size of embedding for ONE categorical feature\n",
        "#     X_num_all.shape[1],\n",
        "#     rtdl.CategoricalFeatureTokenizer(cardinalities, d_token, True, 'uniform'),\n",
        "#     mlp_kwargs,\n",
        "# )\n",
        "# Then the model should be used as `model(x_num, x_cat)` instead of of `model(x)`.\n",
        "\n",
        "model.to(device)\n",
        "optimizer = (\n",
        "    model.make_default_optimizer()\n",
        "    if isinstance(model, rtdl.FTTransformer)\n",
        "    else torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        ")\n",
        "loss_fn = (\n",
        "    F.binary_cross_entropy_with_logits\n",
        "    if task_type == 'binclass'\n",
        "    else F.cross_entropy\n",
        "    if task_type == 'multiclass'\n",
        "    else F.mse_loss\n",
        ")"
      ],
      "id": "AARlx-9Np2SN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxwnRlMNp2SP"
      },
      "source": [
        "### Training"
      ],
      "id": "jxwnRlMNp2SP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBEieBKVp2SP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef6c637-abf6-4bc1-eab3-3c99d22b4dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test score before training: 0.6657\n"
          ]
        }
      ],
      "source": [
        "def apply_model(x_num, x_cat=None):\n",
        "    if isinstance(model, rtdl.FTTransformer):\n",
        "        return model(x_num, x_cat)\n",
        "    elif isinstance(model, (rtdl.MLP, rtdl.ResNet)):\n",
        "        assert x_cat is None\n",
        "        return model(x_num)\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            f'Looks like you are using a custom model: {type(model)}.'\n",
        "            ' Then you have to implement this branch first.'\n",
        "        )\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(part):\n",
        "    model.eval()\n",
        "    prediction = []\n",
        "    for batch in zero.iter_batches(X[part], 1024):\n",
        "        prediction.append(apply_model(batch))\n",
        "    prediction = torch.cat(prediction).squeeze(1).cpu().numpy()\n",
        "    target = y[part].cpu().numpy()\n",
        "\n",
        "\n",
        "    if task_type == 'binclass':\n",
        "        prediction = np.round(scipy.special.expit(prediction))\n",
        "        score = sklearn.metrics.accuracy_score(target, prediction)\n",
        "        score2 = sklearn.metrics.f1_score(target, prediction)\n",
        "    elif task_type == 'multiclass':\n",
        "        prediction = prediction.argmax(1)\n",
        "        score = sklearn.metrics.accuracy_score(target, prediction)\n",
        "    else:\n",
        "        assert task_type == 'regression'\n",
        "        score = sklearn.metrics.mean_squared_error(target, prediction) ** 0.5 * y_std\n",
        "    return score, score2\n",
        "\n",
        "\n",
        "# Create a dataloader for batches of indices\n",
        "# Docs: https://yura52.github.io/zero/reference/api/zero.data.IndexLoader.html\n",
        "batch_size = 256\n",
        "train_loader = zero.data.IndexLoader(len(X['train']), batch_size, device=device)\n",
        "\n",
        "# Create a progress tracker for early stopping\n",
        "# Docs: https://yura52.github.io/zero/reference/api/zero.ProgressTracker.html\n",
        "progress = zero.ProgressTracker(patience=100)\n",
        "\n",
        "print(f'Test score before training: {evaluate(\"test\")[0]:.4f}')"
      ],
      "id": "OBEieBKVp2SP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mw1F4YRSp2SQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7880c790-a46d-4a4e-f9b2-566f0555c98a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(epoch) 1 (batch) 0 (loss) 0.6761\n",
            "(epoch) 1 (batch) 2 (loss) 0.6374\n",
            "(epoch) 1 (batch) 4 (loss) 0.5844\n",
            "(epoch) 1 (batch) 6 (loss) 0.6017\n",
            "(epoch) 1 (batch) 8 (loss) 0.5772\n",
            "(epoch) 1 (batch) 10 (loss) 0.5675\n",
            "(epoch) 1 (batch) 12 (loss) 0.6221\n",
            "Epoch 001 | Validation score: 0.6995 | Test score: accuracy = 0.6970 f1 = 0.5192(epoch) 2 (batch) 0 (loss) 0.5933\n",
            "(epoch) 2 (batch) 2 (loss) 0.6269\n",
            "(epoch) 2 (batch) 4 (loss) 0.5829\n",
            "(epoch) 2 (batch) 6 (loss) 0.5999\n",
            "(epoch) 2 (batch) 8 (loss) 0.5727\n",
            "(epoch) 2 (batch) 10 (loss) 0.5613\n",
            "(epoch) 2 (batch) 12 (loss) 0.6026\n",
            "Epoch 002 | Validation score: 0.6881 | Test score: accuracy = 0.6939 f1 = 0.3674(epoch) 3 (batch) 0 (loss) 0.5978\n",
            "(epoch) 3 (batch) 2 (loss) 0.6297\n",
            "(epoch) 3 (batch) 4 (loss) 0.5956\n",
            "(epoch) 3 (batch) 6 (loss) 0.6082\n",
            "(epoch) 3 (batch) 8 (loss) 0.5672\n",
            "(epoch) 3 (batch) 10 (loss) 0.5594\n",
            "(epoch) 3 (batch) 12 (loss) 0.6067\n",
            "Epoch 003 | Validation score: 0.6705 | Test score: accuracy = 0.6758 f1 = 0.1955(epoch) 4 (batch) 0 (loss) 0.6096\n",
            "(epoch) 4 (batch) 2 (loss) 0.6428\n",
            "(epoch) 4 (batch) 4 (loss) 0.5585\n",
            "(epoch) 4 (batch) 6 (loss) 0.6021\n",
            "(epoch) 4 (batch) 8 (loss) 0.5742\n",
            "(epoch) 4 (batch) 10 (loss) 0.5699\n",
            "(epoch) 4 (batch) 12 (loss) 0.5919\n",
            "Epoch 004 | Validation score: 0.6907 | Test score: accuracy = 0.6929 f1 = 0.3992(epoch) 5 (batch) 0 (loss) 0.5950\n",
            "(epoch) 5 (batch) 2 (loss) 0.6346\n",
            "(epoch) 5 (batch) 4 (loss) 0.5556\n",
            "(epoch) 5 (batch) 6 (loss) 0.5851\n",
            "(epoch) 5 (batch) 8 (loss) 0.5676\n",
            "(epoch) 5 (batch) 10 (loss) 0.5600\n",
            "(epoch) 5 (batch) 12 (loss) 0.5987\n",
            "Epoch 005 | Validation score: 0.7033 | Test score: accuracy = 0.6980 f1 = 0.4348(epoch) 6 (batch) 0 (loss) 0.5888\n",
            "(epoch) 6 (batch) 2 (loss) 0.6312\n",
            "(epoch) 6 (batch) 4 (loss) 0.5619\n",
            "(epoch) 6 (batch) 6 (loss) 0.5911\n",
            "(epoch) 6 (batch) 8 (loss) 0.5609\n",
            "(epoch) 6 (batch) 10 (loss) 0.5543\n",
            "(epoch) 6 (batch) 12 (loss) 0.6133\n",
            "Epoch 006 | Validation score: 0.6869 | Test score: accuracy = 0.6929 f1 = 0.3846(epoch) 7 (batch) 0 (loss) 0.6028\n",
            "(epoch) 7 (batch) 2 (loss) 0.6388\n",
            "(epoch) 7 (batch) 4 (loss) 0.5510\n",
            "(epoch) 7 (batch) 6 (loss) 0.5972\n",
            "(epoch) 7 (batch) 8 (loss) 0.5749\n",
            "(epoch) 7 (batch) 10 (loss) 0.5547\n",
            "(epoch) 7 (batch) 12 (loss) 0.6016\n",
            "Epoch 007 | Validation score: 0.6944 | Test score: accuracy = 0.6909 f1 = 0.3678(epoch) 8 (batch) 0 (loss) 0.5993\n",
            "(epoch) 8 (batch) 2 (loss) 0.6372\n",
            "(epoch) 8 (batch) 4 (loss) 0.5565\n",
            "(epoch) 8 (batch) 6 (loss) 0.6002\n",
            "(epoch) 8 (batch) 8 (loss) 0.5636\n",
            "(epoch) 8 (batch) 10 (loss) 0.5590\n",
            "(epoch) 8 (batch) 12 (loss) 0.5983\n",
            "Epoch 008 | Validation score: 0.6881 | Test score: accuracy = 0.6909 f1 = 0.3831(epoch) 9 (batch) 0 (loss) 0.5916\n",
            "(epoch) 9 (batch) 2 (loss) 0.6353\n",
            "(epoch) 9 (batch) 4 (loss) 0.5613\n",
            "(epoch) 9 (batch) 6 (loss) 0.5919\n",
            "(epoch) 9 (batch) 8 (loss) 0.5641\n",
            "(epoch) 9 (batch) 10 (loss) 0.5552\n",
            "(epoch) 9 (batch) 12 (loss) 0.6021\n",
            "Epoch 009 | Validation score: 0.6919 | Test score: accuracy = 0.6939 f1 = 0.4024(epoch) 10 (batch) 0 (loss) 0.5952\n",
            "(epoch) 10 (batch) 2 (loss) 0.6257\n",
            "(epoch) 10 (batch) 4 (loss) 0.5520\n",
            "(epoch) 10 (batch) 6 (loss) 0.5978\n",
            "(epoch) 10 (batch) 8 (loss) 0.5659\n",
            "(epoch) 10 (batch) 10 (loss) 0.5626\n",
            "(epoch) 10 (batch) 12 (loss) 0.5929\n",
            "Epoch 010 | Validation score: 0.6919 | Test score: accuracy = 0.6909 f1 = 0.3880(epoch) 11 (batch) 0 (loss) 0.5989\n",
            "(epoch) 11 (batch) 2 (loss) 0.6312\n",
            "(epoch) 11 (batch) 4 (loss) 0.5550\n",
            "(epoch) 11 (batch) 6 (loss) 0.5895\n",
            "(epoch) 11 (batch) 8 (loss) 0.5612\n",
            "(epoch) 11 (batch) 10 (loss) 0.5584\n",
            "(epoch) 11 (batch) 12 (loss) 0.5887\n",
            "Epoch 011 | Validation score: 0.6907 | Test score: accuracy = 0.6909 f1 = 0.3855(epoch) 12 (batch) 0 (loss) 0.5929\n",
            "(epoch) 12 (batch) 2 (loss) 0.6398\n",
            "(epoch) 12 (batch) 4 (loss) 0.5585\n",
            "(epoch) 12 (batch) 6 (loss) 0.5991\n",
            "(epoch) 12 (batch) 8 (loss) 0.5623\n",
            "(epoch) 12 (batch) 10 (loss) 0.5552\n",
            "(epoch) 12 (batch) 12 (loss) 0.5960\n",
            "Epoch 012 | Validation score: 0.6944 | Test score: accuracy = 0.6859 f1 = 0.3453(epoch) 13 (batch) 0 (loss) 0.5973\n",
            "(epoch) 13 (batch) 2 (loss) 0.6280\n",
            "(epoch) 13 (batch) 4 (loss) 0.5545\n",
            "(epoch) 13 (batch) 6 (loss) 0.5967\n",
            "(epoch) 13 (batch) 8 (loss) 0.5612\n",
            "(epoch) 13 (batch) 10 (loss) 0.5530\n",
            "(epoch) 13 (batch) 12 (loss) 0.5934\n",
            "Epoch 013 | Validation score: 0.6970 | Test score: accuracy = 0.6859 f1 = 0.3640(epoch) 14 (batch) 0 (loss) 0.5971\n",
            "(epoch) 14 (batch) 2 (loss) 0.6263\n",
            "(epoch) 14 (batch) 4 (loss) 0.5612\n",
            "(epoch) 14 (batch) 6 (loss) 0.5926\n",
            "(epoch) 14 (batch) 8 (loss) 0.5617\n",
            "(epoch) 14 (batch) 10 (loss) 0.5545\n",
            "(epoch) 14 (batch) 12 (loss) 0.6010\n",
            "Epoch 014 | Validation score: 0.6894 | Test score: accuracy = 0.6889 f1 = 0.3447(epoch) 15 (batch) 0 (loss) 0.5935\n",
            "(epoch) 15 (batch) 2 (loss) 0.6273\n",
            "(epoch) 15 (batch) 4 (loss) 0.5545\n",
            "(epoch) 15 (batch) 6 (loss) 0.5968\n",
            "(epoch) 15 (batch) 8 (loss) 0.5703\n",
            "(epoch) 15 (batch) 10 (loss) 0.5570\n",
            "(epoch) 15 (batch) 12 (loss) 0.5983\n",
            "Epoch 015 | Validation score: 0.6944 | Test score: accuracy = 0.6879 f1 = 0.3268(epoch) 16 (batch) 0 (loss) 0.5950\n",
            "(epoch) 16 (batch) 2 (loss) 0.6315\n",
            "(epoch) 16 (batch) 4 (loss) 0.5571\n",
            "(epoch) 16 (batch) 6 (loss) 0.5910\n",
            "(epoch) 16 (batch) 8 (loss) 0.5654\n",
            "(epoch) 16 (batch) 10 (loss) 0.5590\n",
            "(epoch) 16 (batch) 12 (loss) 0.5877\n",
            "Epoch 016 | Validation score: 0.6982 | Test score: accuracy = 0.6899 f1 = 0.3823(epoch) 17 (batch) 0 (loss) 0.5917\n",
            "(epoch) 17 (batch) 2 (loss) 0.6268\n",
            "(epoch) 17 (batch) 4 (loss) 0.5550\n",
            "(epoch) 17 (batch) 6 (loss) 0.5904\n",
            "(epoch) 17 (batch) 8 (loss) 0.5631\n",
            "(epoch) 17 (batch) 10 (loss) 0.5478\n",
            "(epoch) 17 (batch) 12 (loss) 0.5865\n",
            "Epoch 017 | Validation score: 0.6907 | Test score: accuracy = 0.6899 f1 = 0.3722(epoch) 18 (batch) 0 (loss) 0.5986\n",
            "(epoch) 18 (batch) 2 (loss) 0.6218\n",
            "(epoch) 18 (batch) 4 (loss) 0.5514\n",
            "(epoch) 18 (batch) 6 (loss) 0.5965\n",
            "(epoch) 18 (batch) 8 (loss) 0.5614\n",
            "(epoch) 18 (batch) 10 (loss) 0.5499\n",
            "(epoch) 18 (batch) 12 (loss) 0.5979\n",
            "Epoch 018 | Validation score: 0.6894 | Test score: accuracy = 0.6879 f1 = 0.3495(epoch) 19 (batch) 0 (loss) 0.5964\n",
            "(epoch) 19 (batch) 2 (loss) 0.6194\n",
            "(epoch) 19 (batch) 4 (loss) 0.5557\n",
            "(epoch) 19 (batch) 6 (loss) 0.5906\n",
            "(epoch) 19 (batch) 8 (loss) 0.5598\n",
            "(epoch) 19 (batch) 10 (loss) 0.5512\n",
            "(epoch) 19 (batch) 12 (loss) 0.5901\n",
            "Epoch 019 | Validation score: 0.6907 | Test score: accuracy = 0.6899 f1 = 0.3537(epoch) 20 (batch) 0 (loss) 0.5908\n",
            "(epoch) 20 (batch) 2 (loss) 0.6181\n",
            "(epoch) 20 (batch) 4 (loss) 0.5566\n",
            "(epoch) 20 (batch) 6 (loss) 0.5903\n",
            "(epoch) 20 (batch) 8 (loss) 0.5567\n",
            "(epoch) 20 (batch) 10 (loss) 0.5542\n",
            "(epoch) 20 (batch) 12 (loss) 0.5856\n",
            "Epoch 020 | Validation score: 0.6982 | Test score: accuracy = 0.6889 f1 = 0.3714(epoch) 21 (batch) 0 (loss) 0.5844\n",
            "(epoch) 21 (batch) 2 (loss) 0.6289\n",
            "(epoch) 21 (batch) 4 (loss) 0.5562\n",
            "(epoch) 21 (batch) 6 (loss) 0.5853\n",
            "(epoch) 21 (batch) 8 (loss) 0.5609\n",
            "(epoch) 21 (batch) 10 (loss) 0.5509\n",
            "(epoch) 21 (batch) 12 (loss) 0.5868\n",
            "Epoch 021 | Validation score: 0.6944 | Test score: accuracy = 0.6960 f1 = 0.3968(epoch) 22 (batch) 0 (loss) 0.5987\n",
            "(epoch) 22 (batch) 2 (loss) 0.6247\n",
            "(epoch) 22 (batch) 4 (loss) 0.5534\n",
            "(epoch) 22 (batch) 6 (loss) 0.5851\n",
            "(epoch) 22 (batch) 8 (loss) 0.5653\n",
            "(epoch) 22 (batch) 10 (loss) 0.5556\n",
            "(epoch) 22 (batch) 12 (loss) 0.5937\n",
            "Epoch 022 | Validation score: 0.6932 | Test score: accuracy = 0.6949 f1 = 0.3629(epoch) 23 (batch) 0 (loss) 0.5992\n",
            "(epoch) 23 (batch) 2 (loss) 0.6230\n",
            "(epoch) 23 (batch) 4 (loss) 0.5623\n",
            "(epoch) 23 (batch) 6 (loss) 0.5914\n",
            "(epoch) 23 (batch) 8 (loss) 0.5578\n",
            "(epoch) 23 (batch) 10 (loss) 0.5588\n",
            "(epoch) 23 (batch) 12 (loss) 0.5789\n",
            "Epoch 023 | Validation score: 0.6932 | Test score: accuracy = 0.6929 f1 = 0.3640(epoch) 24 (batch) 0 (loss) 0.5894\n",
            "(epoch) 24 (batch) 2 (loss) 0.6309\n",
            "(epoch) 24 (batch) 4 (loss) 0.5543\n",
            "(epoch) 24 (batch) 6 (loss) 0.5923\n",
            "(epoch) 24 (batch) 8 (loss) 0.5563\n",
            "(epoch) 24 (batch) 10 (loss) 0.5620\n",
            "(epoch) 24 (batch) 12 (loss) 0.5854\n",
            "Epoch 024 | Validation score: 0.6995 | Test score: accuracy = 0.6899 f1 = 0.3747(epoch) 25 (batch) 0 (loss) 0.5933\n",
            "(epoch) 25 (batch) 2 (loss) 0.6192\n",
            "(epoch) 25 (batch) 4 (loss) 0.5540\n",
            "(epoch) 25 (batch) 6 (loss) 0.5829\n",
            "(epoch) 25 (batch) 8 (loss) 0.5578\n",
            "(epoch) 25 (batch) 10 (loss) 0.5556\n",
            "(epoch) 25 (batch) 12 (loss) 0.5708\n",
            "Epoch 025 | Validation score: 0.6957 | Test score: accuracy = 0.6960 f1 = 0.4086(epoch) 26 (batch) 0 (loss) 0.5914\n",
            "(epoch) 26 (batch) 2 (loss) 0.6220\n",
            "(epoch) 26 (batch) 4 (loss) 0.5487\n",
            "(epoch) 26 (batch) 6 (loss) 0.5890\n",
            "(epoch) 26 (batch) 8 (loss) 0.5507\n",
            "(epoch) 26 (batch) 10 (loss) 0.5498\n",
            "(epoch) 26 (batch) 12 (loss) 0.5913\n",
            "Epoch 026 | Validation score: 0.6932 | Test score: accuracy = 0.6899 f1 = 0.3644(epoch) 27 (batch) 0 (loss) 0.5985\n",
            "(epoch) 27 (batch) 2 (loss) 0.6188\n",
            "(epoch) 27 (batch) 4 (loss) 0.5505\n",
            "(epoch) 27 (batch) 6 (loss) 0.5857\n",
            "(epoch) 27 (batch) 8 (loss) 0.5562\n",
            "(epoch) 27 (batch) 10 (loss) 0.5568\n",
            "(epoch) 27 (batch) 12 (loss) 0.5784\n",
            "Epoch 027 | Validation score: 0.6881 | Test score: accuracy = 0.6990 f1 = 0.3632(epoch) 28 (batch) 0 (loss) 0.5836\n",
            "(epoch) 28 (batch) 2 (loss) 0.6171\n",
            "(epoch) 28 (batch) 4 (loss) 0.5599\n",
            "(epoch) 28 (batch) 6 (loss) 0.5923\n",
            "(epoch) 28 (batch) 8 (loss) 0.5534\n",
            "(epoch) 28 (batch) 10 (loss) 0.5581\n",
            "(epoch) 28 (batch) 12 (loss) 0.5758\n",
            "Epoch 028 | Validation score: 0.6894 | Test score: accuracy = 0.6970 f1 = 0.3671(epoch) 29 (batch) 0 (loss) 0.5931\n",
            "(epoch) 29 (batch) 2 (loss) 0.6140\n",
            "(epoch) 29 (batch) 4 (loss) 0.5513\n",
            "(epoch) 29 (batch) 6 (loss) 0.5948\n",
            "(epoch) 29 (batch) 8 (loss) 0.5565\n",
            "(epoch) 29 (batch) 10 (loss) 0.5540\n",
            "(epoch) 29 (batch) 12 (loss) 0.5809\n",
            "Epoch 029 | Validation score: 0.6881 | Test score: accuracy = 0.7000 f1 = 0.3721(epoch) 30 (batch) 0 (loss) 0.5956\n",
            "(epoch) 30 (batch) 2 (loss) 0.6140\n",
            "(epoch) 30 (batch) 4 (loss) 0.5553\n",
            "(epoch) 30 (batch) 6 (loss) 0.5849\n",
            "(epoch) 30 (batch) 8 (loss) 0.5574\n",
            "(epoch) 30 (batch) 10 (loss) 0.5633\n",
            "(epoch) 30 (batch) 12 (loss) 0.5683\n",
            "Epoch 030 | Validation score: 0.6919 | Test score: accuracy = 0.6960 f1 = 0.3742(epoch) 31 (batch) 0 (loss) 0.5891\n",
            "(epoch) 31 (batch) 2 (loss) 0.6166\n",
            "(epoch) 31 (batch) 4 (loss) 0.5453\n",
            "(epoch) 31 (batch) 6 (loss) 0.5834\n",
            "(epoch) 31 (batch) 8 (loss) 0.5606\n",
            "(epoch) 31 (batch) 10 (loss) 0.5557\n",
            "(epoch) 31 (batch) 12 (loss) 0.5841\n",
            "Epoch 031 | Validation score: 0.6932 | Test score: accuracy = 0.6970 f1 = 0.3852(epoch) 32 (batch) 0 (loss) 0.5934\n",
            "(epoch) 32 (batch) 2 (loss) 0.6171\n",
            "(epoch) 32 (batch) 4 (loss) 0.5559\n",
            "(epoch) 32 (batch) 6 (loss) 0.5838\n",
            "(epoch) 32 (batch) 8 (loss) 0.5536\n",
            "(epoch) 32 (batch) 10 (loss) 0.5531\n",
            "(epoch) 32 (batch) 12 (loss) 0.5707\n",
            "Epoch 032 | Validation score: 0.6856 | Test score: accuracy = 0.7000 f1 = 0.3613(epoch) 33 (batch) 0 (loss) 0.5936\n",
            "(epoch) 33 (batch) 2 (loss) 0.6054\n",
            "(epoch) 33 (batch) 4 (loss) 0.5574\n",
            "(epoch) 33 (batch) 6 (loss) 0.5907\n",
            "(epoch) 33 (batch) 8 (loss) 0.5606\n",
            "(epoch) 33 (batch) 10 (loss) 0.5610\n",
            "(epoch) 33 (batch) 12 (loss) 0.5686\n",
            "Epoch 033 | Validation score: 0.6881 | Test score: accuracy = 0.6939 f1 = 0.3854(epoch) 34 (batch) 0 (loss) 0.5913\n",
            "(epoch) 34 (batch) 2 (loss) 0.6290\n",
            "(epoch) 34 (batch) 4 (loss) 0.5439\n",
            "(epoch) 34 (batch) 6 (loss) 0.5930\n",
            "(epoch) 34 (batch) 8 (loss) 0.5539\n",
            "(epoch) 34 (batch) 10 (loss) 0.5518\n",
            "(epoch) 34 (batch) 12 (loss) 0.5685\n",
            "Epoch 034 | Validation score: 0.6907 | Test score: accuracy = 0.7020 f1 = 0.3841(epoch) 35 (batch) 0 (loss) 0.5869\n",
            "(epoch) 35 (batch) 2 (loss) 0.6161\n",
            "(epoch) 35 (batch) 4 (loss) 0.5563\n",
            "(epoch) 35 (batch) 6 (loss) 0.5921\n",
            "(epoch) 35 (batch) 8 (loss) 0.5511\n",
            "(epoch) 35 (batch) 10 (loss) 0.5598\n",
            "(epoch) 35 (batch) 12 (loss) 0.5741\n",
            "Epoch 035 | Validation score: 0.6907 | Test score: accuracy = 0.7051 f1 = 0.3917(epoch) 36 (batch) 0 (loss) 0.5927\n",
            "(epoch) 36 (batch) 2 (loss) 0.6080\n",
            "(epoch) 36 (batch) 4 (loss) 0.5616\n",
            "(epoch) 36 (batch) 6 (loss) 0.5862\n",
            "(epoch) 36 (batch) 8 (loss) 0.5540\n",
            "(epoch) 36 (batch) 10 (loss) 0.5544\n",
            "(epoch) 36 (batch) 12 (loss) 0.5714\n",
            "Epoch 036 | Validation score: 0.6856 | Test score: accuracy = 0.6990 f1 = 0.3868(epoch) 37 (batch) 0 (loss) 0.5886\n",
            "(epoch) 37 (batch) 2 (loss) 0.6088\n",
            "(epoch) 37 (batch) 4 (loss) 0.5553\n",
            "(epoch) 37 (batch) 6 (loss) 0.5849\n",
            "(epoch) 37 (batch) 8 (loss) 0.5581\n",
            "(epoch) 37 (batch) 10 (loss) 0.5501\n",
            "(epoch) 37 (batch) 12 (loss) 0.5661\n",
            "Epoch 037 | Validation score: 0.6907 | Test score: accuracy = 0.6990 f1 = 0.3968(epoch) 38 (batch) 0 (loss) 0.5900\n",
            "(epoch) 38 (batch) 2 (loss) 0.6142\n",
            "(epoch) 38 (batch) 4 (loss) 0.5492\n",
            "(epoch) 38 (batch) 6 (loss) 0.5718\n",
            "(epoch) 38 (batch) 8 (loss) 0.5518\n",
            "(epoch) 38 (batch) 10 (loss) 0.5616\n",
            "(epoch) 38 (batch) 12 (loss) 0.5759\n",
            "Epoch 038 | Validation score: 0.6881 | Test score: accuracy = 0.6990 f1 = 0.3893(epoch) 39 (batch) 0 (loss) 0.5911\n",
            "(epoch) 39 (batch) 2 (loss) 0.6208\n",
            "(epoch) 39 (batch) 4 (loss) 0.5513\n",
            "(epoch) 39 (batch) 6 (loss) 0.5797\n",
            "(epoch) 39 (batch) 8 (loss) 0.5595\n",
            "(epoch) 39 (batch) 10 (loss) 0.5576\n",
            "(epoch) 39 (batch) 12 (loss) 0.5515\n",
            "Epoch 039 | Validation score: 0.6869 | Test score: accuracy = 0.7081 f1 = 0.3916(epoch) 40 (batch) 0 (loss) 0.5878\n",
            "(epoch) 40 (batch) 2 (loss) 0.6051\n",
            "(epoch) 40 (batch) 4 (loss) 0.5504\n",
            "(epoch) 40 (batch) 6 (loss) 0.5727\n",
            "(epoch) 40 (batch) 8 (loss) 0.5568\n",
            "(epoch) 40 (batch) 10 (loss) 0.5608\n",
            "(epoch) 40 (batch) 12 (loss) 0.5631\n",
            "Epoch 040 | Validation score: 0.6907 | Test score: accuracy = 0.6970 f1 = 0.3902(epoch) 41 (batch) 0 (loss) 0.5963\n",
            "(epoch) 41 (batch) 2 (loss) 0.6091\n",
            "(epoch) 41 (batch) 4 (loss) 0.5531\n",
            "(epoch) 41 (batch) 6 (loss) 0.5856\n",
            "(epoch) 41 (batch) 8 (loss) 0.5538\n",
            "(epoch) 41 (batch) 10 (loss) 0.5536\n",
            "(epoch) 41 (batch) 12 (loss) 0.5524\n",
            "Epoch 041 | Validation score: 0.6881 | Test score: accuracy = 0.6990 f1 = 0.3943(epoch) 42 (batch) 0 (loss) 0.5891\n",
            "(epoch) 42 (batch) 2 (loss) 0.5958\n",
            "(epoch) 42 (batch) 4 (loss) 0.5589\n",
            "(epoch) 42 (batch) 6 (loss) 0.5795\n",
            "(epoch) 42 (batch) 8 (loss) 0.5470\n",
            "(epoch) 42 (batch) 10 (loss) 0.5599\n",
            "(epoch) 42 (batch) 12 (loss) 0.5660\n",
            "Epoch 042 | Validation score: 0.6957 | Test score: accuracy = 0.7040 f1 = 0.3805(epoch) 43 (batch) 0 (loss) 0.6006\n",
            "(epoch) 43 (batch) 2 (loss) 0.6039\n",
            "(epoch) 43 (batch) 4 (loss) 0.5514\n",
            "(epoch) 43 (batch) 6 (loss) 0.5766\n",
            "(epoch) 43 (batch) 8 (loss) 0.5511\n",
            "(epoch) 43 (batch) 10 (loss) 0.5612\n",
            "(epoch) 43 (batch) 12 (loss) 0.5560\n",
            "Epoch 043 | Validation score: 0.6944 | Test score: accuracy = 0.6960 f1 = 0.3968(epoch) 44 (batch) 0 (loss) 0.5937\n",
            "(epoch) 44 (batch) 2 (loss) 0.6125\n",
            "(epoch) 44 (batch) 4 (loss) 0.5526\n",
            "(epoch) 44 (batch) 6 (loss) 0.5800\n",
            "(epoch) 44 (batch) 8 (loss) 0.5502\n",
            "(epoch) 44 (batch) 10 (loss) 0.5564\n",
            "(epoch) 44 (batch) 12 (loss) 0.5578\n",
            "Epoch 044 | Validation score: 0.6944 | Test score: accuracy = 0.6990 f1 = 0.3817(epoch) 45 (batch) 0 (loss) 0.5913\n",
            "(epoch) 45 (batch) 2 (loss) 0.6097\n",
            "(epoch) 45 (batch) 4 (loss) 0.5502\n",
            "(epoch) 45 (batch) 6 (loss) 0.5737\n",
            "(epoch) 45 (batch) 8 (loss) 0.5486\n",
            "(epoch) 45 (batch) 10 (loss) 0.5451\n",
            "(epoch) 45 (batch) 12 (loss) 0.5500\n",
            "Epoch 045 | Validation score: 0.6894 | Test score: accuracy = 0.7020 f1 = 0.3867(epoch) 46 (batch) 0 (loss) 0.5879\n",
            "(epoch) 46 (batch) 2 (loss) 0.6135\n",
            "(epoch) 46 (batch) 4 (loss) 0.5507\n",
            "(epoch) 46 (batch) 6 (loss) 0.5848\n",
            "(epoch) 46 (batch) 8 (loss) 0.5452\n",
            "(epoch) 46 (batch) 10 (loss) 0.5679\n",
            "(epoch) 46 (batch) 12 (loss) 0.5640\n",
            "Epoch 046 | Validation score: 0.6932 | Test score: accuracy = 0.6990 f1 = 0.3968(epoch) 47 (batch) 0 (loss) 0.5928\n",
            "(epoch) 47 (batch) 2 (loss) 0.6152\n",
            "(epoch) 47 (batch) 4 (loss) 0.5428\n",
            "(epoch) 47 (batch) 6 (loss) 0.5830\n",
            "(epoch) 47 (batch) 8 (loss) 0.5439\n",
            "(epoch) 47 (batch) 10 (loss) 0.5546\n",
            "(epoch) 47 (batch) 12 (loss) 0.5455\n",
            "Epoch 047 | Validation score: 0.6982 | Test score: accuracy = 0.7000 f1 = 0.4048(epoch) 48 (batch) 0 (loss) 0.5825\n",
            "(epoch) 48 (batch) 2 (loss) 0.6181\n",
            "(epoch) 48 (batch) 4 (loss) 0.5549\n",
            "(epoch) 48 (batch) 6 (loss) 0.5703\n",
            "(epoch) 48 (batch) 8 (loss) 0.5464\n",
            "(epoch) 48 (batch) 10 (loss) 0.5494\n",
            "(epoch) 48 (batch) 12 (loss) 0.5467\n",
            "Epoch 048 | Validation score: 0.6907 | Test score: accuracy = 0.7091 f1 = 0.3950(epoch) 49 (batch) 0 (loss) 0.5913\n",
            "(epoch) 49 (batch) 2 (loss) 0.6035\n",
            "(epoch) 49 (batch) 4 (loss) 0.5561\n",
            "(epoch) 49 (batch) 6 (loss) 0.5708\n",
            "(epoch) 49 (batch) 8 (loss) 0.5473\n",
            "(epoch) 49 (batch) 10 (loss) 0.5566\n",
            "(epoch) 49 (batch) 12 (loss) 0.5438\n",
            "Epoch 049 | Validation score: 0.6995 | Test score: accuracy = 0.7040 f1 = 0.4081(epoch) 50 (batch) 0 (loss) 0.5923\n",
            "(epoch) 50 (batch) 2 (loss) 0.6133\n",
            "(epoch) 50 (batch) 4 (loss) 0.5525\n",
            "(epoch) 50 (batch) 6 (loss) 0.5711\n",
            "(epoch) 50 (batch) 8 (loss) 0.5473\n",
            "(epoch) 50 (batch) 10 (loss) 0.5522\n",
            "(epoch) 50 (batch) 12 (loss) 0.5447\n",
            "Epoch 050 | Validation score: 0.6995 | Test score: accuracy = 0.7061 f1 = 0.4305"
          ]
        }
      ],
      "source": [
        "n_epochs = 50\n",
        "report_frequency = len(X['train']) // batch_size // 5\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    for iteration, batch_idx in enumerate(train_loader):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        x_batch = X['train'][batch_idx]\n",
        "        y_batch = y['train'][batch_idx]\n",
        "        loss = loss_fn(apply_model(x_batch).squeeze(1), y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if iteration % report_frequency == 0:\n",
        "            print(f'(epoch) {epoch} (batch) {iteration} (loss) {loss.item():.4f}')\n",
        "\n",
        "    val_score, _ = evaluate('val')\n",
        "    test_score, f1 = evaluate('test')\n",
        "    print(f'Epoch {epoch:03d} | Validation score: {val_score:.4f} | Test score: accuracy = {test_score:.4f} f1 = {f1:.4f}', end='')\n",
        "    # progress.update((-1 if task_type == 'regression' else 1) * val_score)\n",
        "    # if progress.success:\n",
        "    #     print(' <<< BEST VALIDATION EPOCH', end='')\n",
        "    # print()\n",
        "    # if progress.fail:\n",
        "    #     break"
      ],
      "id": "Mw1F4YRSp2SQ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "rtdl.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}